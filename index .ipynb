{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9004f002-b452-4e8d-be83-36612f6e5075",
   "metadata": {},
   "source": [
    "# *KEEPING CUSTOMERS CONNECTED - AND NOT DISCONNECTED!* \n",
    " ## THE SYRIATEL ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a710b482-2708-4b47-b7cc-606fba7dce46",
   "metadata": {},
   "source": [
    "# 1.BUSINESS UNDERSTANDING\n",
    "\n",
    "## **1.1 BUSINESS OVERVIEW**\n",
    "\n",
    "According to this [article](https://www.sciencedirect.com/topics/social-sciences/telecommunications-industry) published in 2011, Telecommunications company is an organization that provides services for long distance communication. They do this by building and mainatining  the physical networks, like cell towers, that transmit signals to individuals and businesses.These companies facilitate essential services like accessing the internet, making phone calls and sending messages. They make money through customer subscriptions and usage fees for these services.SyriaTel is a telecom company that provides call,text and data services to customers. \n",
    "One advantage of working with in the telecommunication sector is that it is a high-performing sector that contributes to economic growth, potentially increasing returns for investors. Telecommunication is also an essential service with steady demand, making it stable and a valuable industry to be part of.\n",
    "However, the telecom industry is highly competitive and customers can easily switch to other providers if they're dissatisfied. This creates a high risk of customer churn, which can reduce revenue and can discourage investor confidence if not properly managed.\n",
    "\n",
    "<img src=\"telecomm.webp\" alt=\"Churn Heatmap\" width=\"600\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **1.2 PROBLEM STATEMENT**\n",
    "SyriaTel is losing customers to competitors, by analysing customer data, we can predict churn and uncover the reasons why customers leave, so SyriaTel can take action to reduce churn and improve customer retention.\n",
    "\n",
    "This is costly because:\n",
    "\n",
    "*Revenue loss:* Each customer lost means recurring revenue lost.\n",
    "\n",
    "*High acquisition cost:* It is more expensive to acquire a new customer than to retain an exsisting one.\n",
    "\n",
    "*Competitive pressure:* In a competetive market, reducing churn is critical for survival and growth.\n",
    "\n",
    "If we can predict which customers are likely to leave, SyriaTel can take action early eg. giving offers, improving services,or solving problems to make those customers stay.\n",
    "\n",
    "So the goal is to reduce churn and keep loyal customers.\n",
    "\n",
    "\n",
    "## **1.3 BUSINESS OBJECTIVES**\n",
    "\n",
    " ## 1.3.1 *Main objective:*\n",
    "To predict customer churn and provide insights that help SyriaTel keep its customers and reduce revenue loss.\n",
    "\n",
    " ## 1.3.2 *Specific objectives:*\n",
    "\n",
    "1. To develop a model that predicts whether a customer will churn or stay.\n",
    "2. To identify the key factors eg. call charges, service quality or customer complaints that influence the probability of a customer to churn or not to churn.\n",
    "3. To provide insights that SyriaTel can use to design strategies for reducing churn and improving customer satisfaction.\n",
    "4. To determine the state with the highest churning rate.\n",
    "\n",
    "\n",
    "\n",
    " ## 1.3.3*Research questions*\n",
    "1. Can we accurately predict which syriaTel customers are likely to churn using their demographic and usage data?\n",
    "2. What are the main factors that influence customer churn?\n",
    "3. How can SyriaTel use the model's prediction and insights to design strategies that reduce churn and retain more customers?\n",
    "4. What is the state with the highest churning rate?\n",
    "\n",
    "## **1.4 SUCCESS CRITERIA**\n",
    " ***Model performance***\n",
    "The churn prediction model achieves a good level of accuracy and balances correctly identifying customers who churn and those who stay.\n",
    " ***Insights gained***\n",
    "The analysis clearly identifies the key factors that contribute to churn eg. high call charges and frequent complains.\n",
    " ***Business value***\n",
    "SyriaTel can use the model's results to take practical actions, such as designing loyalty offers or improving customer service which can help improve customer churn.\n",
    "\n",
    "\n",
    "# 2. DATA UNDERSTANDING\n",
    "The Syria Tel customer churn dataset we are working with is from [Kaggle](https://www.kaggle.com/becksddf/churn-in-telecoms-dataset).Our data is on Syria Tel which is a telecommunication conmpany, it had a total of 21 columns and 3333 rows after data cleaning we decided to work with the coloumns below: where `churn` is our dependent varaible.\n",
    "    \n",
    "`state` – U.S. state where the customer lives.\n",
    "    \n",
    "`account length `– Number of days the customer has had the account.\n",
    "    \n",
    "`area code` – Telephone area code.\n",
    "    \n",
    "`phone number` – Customer’s phone number (serves as an identifier, not useful for prediction).\n",
    "    \n",
    "`international plan`– Whether the customer has an international calling plan (yes/no).\n",
    "    \n",
    "`log_vmail_messages` – Number of voicemail messages the customer has.\n",
    "    \n",
    "`customer service calls` – Number of calls made to customer service.\n",
    "    \n",
    "**`churn`** – Whether the customer left the company (True = churned, False = stayed). which is our dependent variable**\n",
    "    \n",
    "`total_calls` - The total number of calls.\n",
    "    \n",
    "`total_minutes` - The total number number of minutes for all calls.\n",
    "    \n",
    "`total_charge` - The total charges for all calls.\n",
    "\n",
    "     \n",
    "We are merging the columns `total day minutes` ,`total eve minutes` and `total night minutes` into one column named `total_minutes`. We are also merging `total day calls` , `total eve calls` and `total night calls` into one column named `total_calls`. The columns `total day charge`, `total eve charge`,  and`total night charge` are also being merged to become one column called `total_charge`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0af2d-0e2a-41cf-9187-3632dafa9b46",
   "metadata": {},
   "source": [
    "# 3. DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825b1f44-934e-4520-87a7-def308368d5f",
   "metadata": {},
   "source": [
    "## 3.1 Loading a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2b8bb-996d-4182-8b39-7fe932d1ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359ee55c-d30d-4332-81b2-6aed961772e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Syria_Tel.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29b556-33e9-4130-86c6-4c6745594133",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21f6d43-e5e2-469c-b227-04035d527cc5",
   "metadata": {},
   "source": [
    "Our dataset has *3333* rows and *21* columns ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6024c800-4391-44f0-b851-3aad3f8f4ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a02f2a-2ab1-4275-a90c-2bf3c53925e6",
   "metadata": {},
   "source": [
    "## 3.2 Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd9ff3-a88e-400b-ae1a-6a815913acb8",
   "metadata": {},
   "source": [
    "Let's check for any missing values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f3fc4-d6dd-4cc2-9746-bb4e07db2225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd29a4-74c7-4704-956e-c9cf98ec2e53",
   "metadata": {},
   "source": [
    "Since our dataset doesn't have any missing values we don't have to drop any null or fill for missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5662058-3b5d-4153-9b17-fc64969585e4",
   "metadata": {},
   "source": [
    "Let's drop the `phone number` column since it is not useful in our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac97c04a-b84e-4d04-af47-966859784dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping a column\n",
    "df = df.drop(columns =['phone number'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ea4ee-0346-45ff-a1b0-7f15dedafa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edffe56-12a5-42ea-8728-2c3efa5d0855",
   "metadata": {},
   "source": [
    "We need to check for categorical data in our dataset so that we can perform **one hot encoding** which is an important step for us to make predictions and create Machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b53483-cee4-454d-8b44-b5359c52f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for categorical values\n",
    "categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "print(categorical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cc3ccc-d181-44f4-a1a2-2ef0b0cd3f9d",
   "metadata": {},
   "source": [
    "`state`, `international plan` and `voice mail plan` are the columns that are categorical and we need to perfom one hot encoding on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217bb2a6-8a28-4fb4-9ed0-041902c58ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"international plan\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798578de-a786-4f7c-abde-47bc7bdf92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"voice mail plan\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479b85c-5d23-4ffd-bb8b-b46f7b0a756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode both binary categorical columns\n",
    "df_encoded = pd.get_dummies(df,columns=[\"international plan\", \"voice mail plan\"],drop_first=True,dtype=int)\n",
    "df_encoded.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3dbda-79d3-4174-bb9c-cf4be139cd22",
   "metadata": {},
   "source": [
    "Encoding `state` into 50 dummy columns might not be the recommended way to go about it because it will make it harder for the model to interpret and cause **multicollinearity**, So for this `state` column we took a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2751165-7aac-4331-889c-ccb02257d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded[\"state\"].value_counts().head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80122975-9a11-4d4d-944a-bcfee02f5f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded[\"churn\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d5b8d-6e90-4f21-b53b-16e5db515e45",
   "metadata": {},
   "source": [
    "For uniformity i wanna change the contents of this columns to 0 and 1 to match the new encoded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273140b4-4604-4462-8e9d-f8c9950eff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert entries\n",
    "df_encoded[\"churn\"] = df_encoded[\"churn\"].map({True: 1, False: 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c796d34-b986-45fd-8cc3-7714ed45bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded[\"churn\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bdef2a-667c-4454-8706-df499391db32",
   "metadata": {},
   "source": [
    "Let's check for class imbalance in our dependent variable `churn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf311432-e11b-4551-9738-7267890d7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded[\"churn\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de101f6-baf4-4d57-9caf-3e07206dcd39",
   "metadata": {},
   "source": [
    "There is clearly a class imbalance on this column with **85.5%** of the data going to 0 (stay) while the other **14.49%** going to churn and this might not give a correct representation of our model prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698de95b-fee8-4307-ae81-1fb089d96d5f",
   "metadata": {},
   "source": [
    "Let's check for feature distributions and decide whether to perform log transformation or other normalizations before modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47f915b-1142-4285-90f4-f3e341f09d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e968d09-6ae0-46d1-8bba-b504ac3247b6",
   "metadata": {},
   "source": [
    "Every other column seems to have a n even distribution or little to know skewness.But let's take a look at `number vmail messages` whose most customers have 0 messages but some have upto 51 messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282222c6-a492-41dc-99b1-300d216f5395",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_encoded[\"number vmail messages\"], kde=True, bins=30)\n",
    "plt.title(\"Distribution of Total Day Minutes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e7094c-e27d-4e61-b346-f18aa9b6beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded[\"log_vmail_messages\"] = np.log1p(df_encoded[\"number vmail messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f1f9d2-32f2-41cf-88ad-a3933a2eac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_encoded[\"log_vmail_messages\"], kde=True, bins=30)\n",
    "plt.title(\"Log-Transformed Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a0828-c10b-4f31-9eee-e7c3c18076a5",
   "metadata": {},
   "source": [
    " # 3.4 Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e2c1d-df6c-4376-bb37-53e0869ff69e",
   "metadata": {},
   "source": [
    "Let's check how different columns correlate with each other before we decide on what features to use so we need to conduct feature engineering on the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee0607f-1ffa-4054-a321-1acf5430af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's check for multicollinearity\n",
    "corr = df_encoded.corr(numeric_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86040b2f-02f4-4512-8a2a-7fc6465cc495",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"RdBu_r\", center=0, cbar=True)\n",
    "plt.title(\"Correlation Heatmap of All Numeric Features\",fontsize = 14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc1477-f6d8-455c-be26-2f9df68120b5",
   "metadata": {},
   "source": [
    "Some of this features are highly correlated and give the same insights for example `total day minutes`,`total eve minutes`and `total night minutes` can be put together to give us `total_minutes` that are used in a 24hrs.This will be similarly be applied to `total_calls` and `total_charge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb0e50-d3f8-417d-ba38-f0bc10ce2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded[\"total_minutes\"] = df_encoded[\"total day minutes\"] + df_encoded[\"total eve minutes\"] + df_encoded[\"total night minutes\"]\n",
    "df_encoded[\"total_calls\"] = df_encoded[\"total day calls\"] + df_encoded[\"total eve calls\"] + df_encoded[\"total night calls\"] \n",
    "df_encoded[\"total_charge\"] = df_encoded[\"total day charge\"] + df_encoded[\"total eve charge\"] + df_encoded[\"total night charge\"] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a59e6ff-e0db-4204-aa12-d29c53b4819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop =[\"total day minutes\", \"total eve minutes\", \"total night minutes\", \"total day calls\", \"total eve calls\", \"total night calls\",\n",
    "    \"total day charge\", \"total eve charge\", \"total night charge\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8137db4-1065-4373-ac6a-8a61625591ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_encoded.drop(columns = cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e622f-f5bb-4b65-ac4a-f1c2a806abc2",
   "metadata": {},
   "source": [
    "Since `voice mail plan_yes` and `number vmails messages` carry essentially the same info and have a high correlation of **0.96**.Let's drop the column `voive mail plan_yes` because one can't sent or recieve any voice mails without a plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec919a7-150b-403e-b270-102e4975a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_encoded.drop(columns = [\"voice mail plan_yes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f4d7b-23fd-4235-b926-45008f760403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned dataframe\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd8b3b-a53d-41e6-942d-30d8e3802469",
   "metadata": {},
   "source": [
    "# 4. EXPLANATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd5730-3adf-4511-96f8-642e096bd1f0",
   "metadata": {},
   "source": [
    "Let's do a bit of explanatory data analysis before we move to building our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b02710-356f-4034-b6cf-f73e1c13dd71",
   "metadata": {},
   "source": [
    "## 4.1 Top 5 and Bottom 5 states with highest churn rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed97d2-785e-4f19-8422-fc2611b06640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by state and churn counts\n",
    "state_churn = df_encoded.groupby([\"state\", \"churn\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Add churn rate per state\n",
    "state_churn[\"churn_rate\"] = state_churn[1] / (state_churn[0] + state_churn[1])\n",
    "\n",
    "# Sort by churn rate (descending)\n",
    "highest_churn = state_churn.sort_values(by=\"churn_rate\", ascending=False).head(5)\n",
    "\n",
    "least_churn = state_churn.sort_values(by =\"churn_rate\", ascending=False).tail(5)\n",
    "\n",
    "print(\"Top 5 states with the highest churn rate:\")\n",
    "print(highest_churn)# show top 10 states with highest churn rate\n",
    "print(\"Top 5 states with the least churn rate:\")\n",
    "print(least_churn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e2beb0-ef5a-4d20-8059-2981e3e87a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "highest_churn[\"churn_rate\"].plot(kind=\"barh\", color=\"cyan\")\n",
    "plt.title(\"Churn Rate by State\")\n",
    "plt.ylabel(\"Churn Rate\")\n",
    "plt.xlabel(\"State\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416a404-0608-4715-b49d-bb530aab1e06",
   "metadata": {},
   "source": [
    "**New Jersey (NJ)**: Out of 68 customers, 18 churned giving a 26.5% churn rate.\n",
    "\n",
    "**California (CA)**: Out of 34 customers, 9 churned giving a 26.5% churn rate.\n",
    "\n",
    "**Texas (TX)**: Out of 72 customers, 18 churned this is a 25% churn rate.\n",
    "\n",
    "**Maryland (MD)**: Out of 70 customers, 17 churned which is a 24.3% churn rate.\n",
    "\n",
    "**South Carolina (SC)**: Out of 60 customers, 14 churned equal to 23.3% churn rate.\n",
    "\n",
    "This might be due to reasons such as high competition, customer expectations among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73b9d35-cf3c-45d3-8b6b-f1d28b0e0bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "least_churn[\"churn_rate\"].plot(kind=\"barh\", color=\"pink\")\n",
    "plt.title(\"Churn Rate by State\")\n",
    "plt.ylabel(\"Churn Rate\")\n",
    "plt.xlabel(\"State\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84720a59-e73f-48d5-93a5-d72e994a762b",
   "metadata": {},
   "source": [
    "**Hawaii (HI)**: Out of 53 customers, only 3 churned equal to 5.7% churn rate.\n",
    "\n",
    "**Alaska (AK)**: Out of 52 customers, 3 churned which is  5.8% churn rate.\n",
    "\n",
    "**Arizona (AZ)**: Out of 64 customers, 4 churned adds upto 6.3% churn rate.\n",
    "\n",
    "**Virginia (VA)**: Out of 77 customers, 5 churned equals to 6.5% churn rate.\n",
    "\n",
    "**Iowa (IA)**: Out of 44 customers, 3 churned equals to 6.8% churn rate.\n",
    "\n",
    "This will guide the company to know where it's loyal customers are and where they have a stronger market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d61813-7a11-4fb6-a2f7-498c8e7c8cd5",
   "metadata": {},
   "source": [
    "# 5. MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be5970-dd6b-4bb6-81c8-a1f5e0708c29",
   "metadata": {},
   "source": [
    "## 5.1 BASELINE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08331654-28f7-4c65-8496-8c5d8d992403",
   "metadata": {},
   "source": [
    "### 5.1.1 LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4085e6ab-cb3b-4bf8-9fc5-7a75f71b7f97",
   "metadata": {},
   "source": [
    "Let's move to creating models,specifically a logistic regression model because the problem we are trying to answer is a binary classification and we are also trying to answer the question **what is the probability of a customer to churn or not to churn?** depending on various features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66041b9e-f8ce-43ae-8158-457e18e7b2fd",
   "metadata": {},
   "source": [
    "Before moving to the modelling bit of things we first have to do **feature selection** and also look at our predictor variable `churn`, So as to create our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e9fbaa-0bc6-49f7-a2cc-a1bd698abd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['churn'].value_counts(normalize =True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802aa46-8327-4b38-80f9-3d7ae65765e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"churn\", data=df_encoded,color =\"tomato\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a0f866-3f5f-4eb7-a3a5-ec7391dfb2fa",
   "metadata": {},
   "source": [
    "As you can see from the above visual,the target variable is highly imbalanced.The class 0 has a percentage of **85.5%** while our class 1 has **14.49%** .It shows that 85% of the customers stayed while 14.49% churned which we must address during model training to avoid biased predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a1ccd-f9a4-428b-a505-132788566eac",
   "metadata": {},
   "source": [
    "Since `churn` is binary we can compute **Pearson correlation** between churn and other features before doing Logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bf6513-d44a-4cea-83ff-e9a6ff4f9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep numeric columns\n",
    "numeric_df = df_encoded.select_dtypes(include=[np.number])\n",
    "\n",
    "# Correlation with churn\n",
    "churn_corr = numeric_df.corr()[\"churn\"].sort_values(ascending=False)\n",
    "print(churn_corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22285e35-74b7-499f-b42a-107a95b56327",
   "metadata": {},
   "source": [
    "Let's visualize this correlation to `churn` column which is our dependent feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea2590-5120-4754-8b3a-1a843d0d3aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to Dataframe for heatmap\n",
    "churn_corr_df = churn_corr.to_frame()\n",
    "#plot heatmap\n",
    "plt.figure(figsize=(6,10))\n",
    "sns.heatmap(churn_corr_df, annot=True, cmap=\"coolwarm\", center=0, cbar=True)\n",
    "plt.title(\"Correlation of Features with Churn\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d203ee6-b8a4-490f-9546-3c256fd9330d",
   "metadata": {},
   "source": [
    "We will be using Positive correlation features because they tell us who is likely to churn while Negaative correlation features tell us who is likely to stay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ec88d-7755-4eee-9663-925578c08abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn model import required libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report,roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69ae015-107b-4653-86e6-644a8136581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 4 positive correlatrion with churn\n",
    "selected_features =[\"international plan_yes\",\"total_charge\" ,\"customer service calls\" ,\"total_minutes\"]           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf3255-b411-40a6-82e8-9f5fcbbbce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#independent and dependent variables\n",
    "X = df_encoded[[\"international plan_yes\",\"total_charge\",\"customer service calls\" ,\"total_minutes\"]]          \n",
    "y = df_encoded[\"churn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881e6d7-3fe3-46db-8d32-1655ddb95051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42, stratify=y) #stratify=y ensures the churn ration is preserved in both train and test\n",
    "log_reg = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "#fit the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "#let's predict\n",
    "y_pred = log_reg.predict(X_test)\n",
    "y_proba = log_reg.predict_proba(X_test)[:, 1]  # probability of churn (class 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476d372f-c5a3-4dc1-a2b6-215e9e5f63af",
   "metadata": {},
   "source": [
    "## 5.2 LOGISTIC REGRESSION WITH ALL FEATURES."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39561f15-2917-407a-a3b0-5aaa5473b7ca",
   "metadata": {},
   "source": [
    "We want to check whether adding more information improves the model compared to the baseline.For features we are using all the predictores available in the dataset excluding the target variable. We'll first onehotencode the multi-categorical variable state to have a smooth flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666acfb4-a594-4567-952a-93757a696185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee3983c-86bc-4a43-98ef-7ad7ece27b03",
   "metadata": {},
   "source": [
    "Let's split and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54aeb6-c345-43e1-9c33-8a3d41ff59e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_encoded.drop(\"churn\", axis=1)\n",
    "y = df_encoded[\"churn\"]\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7ba88c-1911-45ad-9a64-a70450db0092",
   "metadata": {},
   "source": [
    "We will have to **OneHotEncode** our multi-categorical variable after conducting a split so that it ensures encoding happens after splitting, with no leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5fa76c-a1c7-4ac9-9a86-70705e897a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode state \n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "# Fit on train, transform both train & test\n",
    "X_train_state = encoder.fit_transform(X_train[[\"state\"]])\n",
    "X_test_state = encoder.transform(X_test[[\"state\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d342c-75e2-47c6-87d9-57a6e4bf9ed6",
   "metadata": {},
   "source": [
    "Now scale the the numeric features after encoding `state`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484ecc88-856a-4547-adb3-5d20d8b8fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all numeric features\n",
    "numeric_features = ['account length', 'area code', 'number vmail messages',\n",
    "       'total intl minutes', 'total intl calls', 'total intl charge','log_vmail_messages',\n",
    "       'customer service calls', 'international plan_yes','total_minutes',\n",
    "       'total_calls', 'total_charge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e4b9e0-1149-404b-acda-31f2f73985cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_num = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test_num = scaler.transform(X_test[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bb1819-a865-4109-9f5b-d77f2fcd2821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine encoded vs numeric \n",
    "X_train_final = np.hstack([X_train_state, X_train_num])\n",
    "X_test_final = np.hstack([X_test_state, X_test_num])\n",
    "\n",
    "#fit the model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42,class_weight=\"balanced\")\n",
    "model.fit(X_train_final, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_final)\n",
    "y_pred_proba = model.predict_proba(X_test_final)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd43c60-bcdb-43a9-a80c-c6c69a66573e",
   "metadata": {},
   "source": [
    "## 5.3 DECISIONTREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c562c54f-18fb-402c-8aa5-5be60b36543d",
   "metadata": {},
   "source": [
    "Let's build another model to perform classification ,in this case a DecisionTree Classifier ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe0215-9b96-4648-929b-6365fd5744a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()\n",
    "df_encoded[\"state_encoded\"] = enc.fit_transform(df_encoded[[\"state\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81faf8-5612-4de2-b6b0-d2c8df23202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_encoded.drop([\"state\"],axis=1)\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed60b57d-7756-4290-8821-0bd2f2158aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop(columns=[\"churn\"])\n",
    "y = df_encoded['churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c053cb7-92ba-435a-8c8c-2c6842a2bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize the model\n",
    "dt = DecisionTreeClassifier(random_state=42, class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83322069-bcca-45f1-ae84-07ced0e3998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter grid\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 5, 7, 10,],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "#GridSearchCV will try different tree depths, splits, and leaf sizes, then pick the best configuration.\n",
    "grid_search = GridSearchCV(estimator=dt,param_grid=param_grid,cv=5,scoring=\"recall\",n_jobs=-1)\n",
    "\n",
    "# Run grid search on training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "y_pred_proba = grid_search.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8764d698-9b91-4b5c-b261-11c50031255c",
   "metadata": {},
   "source": [
    "## 5.3 RANDOM FOREST MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7b6b9-10f6-419e-ac8c-39ac6b0949de",
   "metadata": {},
   "source": [
    "we will advance to use Random forest model to build another model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62010809-07cb-4ce2-8f18-9065eb265e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28189879-3648-4d40-beee-354d6ace9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# churn = target variable\n",
    "\n",
    "X = df_encoded.drop(\"churn\", axis=1)\n",
    "y = df_encoded[\"churn\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea774f3a-c4b1-4793-afab-37da31c8491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42, class_weight=\"balanced\")\n",
    "#\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [5, 10, None],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2]\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"recall\",   # optimize for recall\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    refit=True          # refits the best model on the whole training set\n",
    ")\n",
    "#fit grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a72cc9-879e-404b-ab62-8fa201aad7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_rf.predict(X_test)\n",
    "y_proba = best_rf.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1304f16-488c-482c-b2c6-9e3e0f028cf8",
   "metadata": {},
   "source": [
    "# 6.EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7bd57-e15e-463b-a37f-155720b3e39b",
   "metadata": {},
   "source": [
    "In this section we will be evaluating our models to determine which performs better at predicting churning customers. We will compare the models and ultimately choose the one that performs better as our baseline model of recommendation.Let's begin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0247226-9bc8-4e07-b8bf-122b38cbf642",
   "metadata": {},
   "source": [
    "We will be using Recall and ROC-AUC as the metric of success of our model.we will be using:\n",
    "\n",
    "**Recall** \n",
    "*  Recall measures how many actual churners the model correctly identifies.\n",
    "*  In churn prediction, missing a churner is costly, because it means losing a customer and revenue.\n",
    "*  By optimizing high recall, we ensure the model captures most at-risk customers, even if it occasionally flags a few non-churners.\n",
    "\n",
    "**ROC-AUC**\n",
    "* measures the model’s ability to discriminate between churners and non-churners across all thresholds.\n",
    "* ROC-AUC is threshold-independent, so it evaluates the model’s overall ranking ability.\n",
    "* A high ROC-AUC means the model is reliable in assigning higher churn probabilities to churners than to non-churners, which is critical for making informed business decisions.\n",
    "\n",
    "Togther they align with our business objectives and the problem we are tyrying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a2f24e-7068-4e7a-bd3d-b5c50b5f12dd",
   "metadata": {},
   "source": [
    "## 6.1 Logistic regression baseline model\n",
    "\n",
    "We start with our basemodel logistic with 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc76bdb-5998-46a1-855b-3dbf8c5b35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39de39b1-ee36-4969-8f78-8b4367692c98",
   "metadata": {},
   "source": [
    "**Accuracy score:**\n",
    "\n",
    "We have an accuracy score of 76% which means about 76% of predictions are correct.But because the dataset is imbalanced accuracy alone is misleading.\n",
    "\n",
    "**recall:**\n",
    "\n",
    "The model correctly identifies 75% of true churners meaning the model is good at catching churners even though it misses about 25%.\n",
    "\n",
    "**Precision :**\n",
    "\n",
    "Out of all the customers predicted as churners, only 35% actually churn meaning it gives a high number of false positives.It therefore predicts churn when the customer stays.\n",
    "\n",
    "This model is better at finding churners (high recall) than being precise about them (low precision).\n",
    "This means:\n",
    "\n",
    "We will catch most customers who are likely to churn,but also flag many who wouldn’t have churned (false alarms).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c8673-03ba-4e99-8d34-165ac7a326fc",
   "metadata": {},
   "source": [
    "Let's visualize our ROC-AUC for more understanding of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3858d-cf9d-41a5-8735-3dd079fa8c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# Compute AUC\n",
    "roc_auc_base = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc_base:.2f})\", color=\"tomato\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"black\")  # Random baseline\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.title(\"ROC-AUC Curve for Churn Prediction Model\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed5fe1-3475-4e47-887d-68dc9712298d",
   "metadata": {},
   "source": [
    "Our ROC curve lies well above the diagonal, meaning the model does a good job distinguishing churners from non-churners.It has an AUC = 0.81 which is good and shows a that our model is highly predictive but has room for growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a193ea7-f5f5-430f-90e3-66cb0f5bc7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
    "            xticklabels=[\"Not Churn\", \"Churn\"],\n",
    "            yticklabels=[\"Not Churn\", \"Churn\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3632697a-8d30-464a-b81b-53c3605500f1",
   "metadata": {},
   "source": [
    "Top-left cell: customers correctly predicted as “stay”(True positive).\n",
    "\n",
    "Bottom-right cell: customers correctly predicted as “churn”(True negative).\n",
    "\n",
    "Top-right cell: customers predicted to churn but actually stayed (false positives).\n",
    "\n",
    "Bottom-left cell: customers predicted to stay but actually churned (false negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c41da14-eeed-45a1-839a-aac71523aaf2",
   "metadata": {},
   "source": [
    "**7.1.2 Feature importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff936de0-7a83-4811-a160-bf3dc9cd7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Coefficient\": log_reg.coef_[0]\n",
    "}).sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ada563-5c54-4bf4-ae29-2950d2a08cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# Horizontal bar plot\n",
    "sns.barplot(data =feature_importance,x=\"Coefficient\",y=\"Feature\")\n",
    "plt.title(\"Feature Importance from Logistic Regression\")\n",
    "plt.xlabel(\"Coefficient Value (Impact on Churn)\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aadcde-1463-4099-b76c-e1815037c0e2",
   "metadata": {},
   "source": [
    "**Impact on churn**\n",
    "\n",
    "Let's look at how the features of our baseline model affect churn.\n",
    "\n",
    "`international plan_yes`(2.33): Customers with an international plan are much more likely to churn.\n",
    "\n",
    "`customer service calls`(0.62): The more times a customer calls customer service, the higher the chance they churn.\n",
    "\n",
    "`total_charge` (0.079): As charges increase, churn likelihood slightly increases.weak effect\n",
    "\n",
    "`total_minutes`(-0.0007) :Customers who use more minutes are slightly less likely to churn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a0d7f-81e5-436f-a154-b68f72b2eee9",
   "metadata": {},
   "source": [
    "## 6.2 Logistic regression  model with all features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67840abe-4d19-413e-91b8-0a783d99ba61",
   "metadata": {},
   "source": [
    "Let's first start by checking for **overfitting** in our model since we have used all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3077b41-d79a-4c8b-a1bc-8b2891184b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train predictions\n",
    "y_train_pred = model.predict(X_train_final)\n",
    "y_train_prob = model.predict_proba(X_train_final)[:,1]\n",
    "\n",
    "# Test predictions\n",
    "y_test_pred = model.predict(X_test_final)\n",
    "y_test_prob = model.predict_proba(X_test_final)[:,1]\n",
    "\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "print(\"Train ROC-AUC:\", roc_auc_score(y_train, y_train_prob))\n",
    "print(\"Test ROC-AUC:\", roc_auc_score(y_test, y_test_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a044045e-f70d-4ccf-9375-2facbb1b9df8",
   "metadata": {},
   "source": [
    "our logistic regression with all features does not show strong signs of overfitting. The performance is slightly better on the training set but performs close enough to the test set data suggesting good generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27edc4d4-7810-4c21-8bc6-56b08999d6ef",
   "metadata": {},
   "source": [
    "Let's take a look at the model's general performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e89b9-463b-4e72-b063-fe317ecb116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385289fd-d85f-4022-945f-6faf231c8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[\"No Churn\", \"Churn\"], yticklabels=[\"No Churn\", \"Churn\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1507ab3-845d-4c4d-8795-9c18e608fc5e",
   "metadata": {},
   "source": [
    "## 6.3 DecisionTreeClassifier with all features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5ccf3-89f4-4533-85b8-6db0aa5fc0b1",
   "metadata": {},
   "source": [
    "Our Logistic regression model with all features did not improve.This might be because logistic regression may not fully capture nonlinear patterns in our data so we try a DecisionTreeClassifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7077877-f6bd-4966-8c62-95de776f9ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"ROC-AUC:\",roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d09e22b-a968-458e-ae4f-b23e7feca415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "#roc_auc = roc_auc_score(y_test, y_proba)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "# Compute AUC score\n",
    "roc_auc_tr = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f\"Decision Tree (AUC = {roc_auc_tr:.2f})\")\n",
    "plt.plot([0,1],[0,1],'--',color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.title(\"ROC-AUC Curve - Decision Tree\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2089caa4-ebab-45cf-a168-f4849beb1557",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(X_test)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
    "            xticklabels=[\"No Churn\", \"Churn\"],\n",
    "            yticklabels=[\"No Churn\", \"Churn\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - Decision Tree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b95198-4192-4fcd-b6c4-a2e0ba7a2626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "# Get the best estimator from grid search\n",
    "best_dt = grid_search.best_estimator_\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(20,10))\n",
    "tree.plot_tree(\n",
    "    best_dt, \n",
    "    feature_names=X_train.columns,    # Names of your features\n",
    "    class_names=[\"No Churn\", \"Churn\"],  # Target classes\n",
    "    filled=True,                      # Color nodes by class\n",
    "    rounded=True,                     # Rounded boxes\n",
    "    fontsize=12\n",
    ")\n",
    "plt.title(\"Decision Tree from GridSearchCV\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab4eda-2080-4579-a576-18dd5f07fc8c",
   "metadata": {},
   "source": [
    "The Decision Tree is performing much better across all metrics than the baseline logistic regression.\n",
    "\n",
    "**Recall**: Slightly improved from 0.753 to 0.808 meaning the tree catches more actual churners.\n",
    "\n",
    "**ROC-AUC**: Higher AUC of 0.90 means the tree has a much better ability to discriminate churners from non-churners overall\n",
    "\n",
    "The Decision Tree clearly outperforms the baseline logistic regression on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6864bc-4656-4b2d-8ce2-34daa1850947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "feat_importance = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Importance\": best_dt.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(feat_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06efd1-7289-4a8c-97f6-35fa82f2c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top features\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feat_importance[\"Feature\"], feat_importance[\"Importance\"], color=\"royalblue\")\n",
    "plt.gca().invert_yaxis()  # largest on top\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Feature Importance - Decision Tree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115705cf-2f4a-404f-88df-1fa5244f795b",
   "metadata": {},
   "source": [
    "**top 3 features** \n",
    "\n",
    "`total_charge` at a coefficient of (0.34) is strongest predictor of churn this can be intepreted as higher charges may indicate higher risk or dissatisfaction.\n",
    "\n",
    "`customer service calls`(0.21) Number of calls to customer service strongly signals churn this might be a likelihood of more complaints.\n",
    "\n",
    "`international plan_yes`(0.195) Having an international plan contributes significantly to predicting churn this might be perhaps because of cost concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d95c0-43dc-46be-ba58-27c76f8274c4",
   "metadata": {},
   "source": [
    "## 6.4 Random forest with all features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffe64c-c048-4b6a-ab58-c319fc838791",
   "metadata": {},
   "source": [
    "Let's take a look at how our Random Forest model worked compared to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f118ce-706d-4a33-b889-4e2cdfa400bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82423d-daeb-4253-8e47-983582e617cb",
   "metadata": {},
   "source": [
    "Random Forest outperforms both the baseline logistic regression and the Decision Tree in accuracy, precision, F1-score, and ROC-AUC.\n",
    "\n",
    "**Recall** is the ability to detect actual churners and is slightly lower than the tuned Decision Tree (0.807 vs 0.814), but the precision and overall F1 improved significantly, meaning fewer false positives.\n",
    "\n",
    "**ROC-AUC** Random Forest has the highest ROC-AUC (0.924), indicating it discriminates churners from non-churners better than the other models.\n",
    "\n",
    "Random Forest gives the best overall performance on this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be41ce57-1cef-4635-88be-cd81ae04e7ea",
   "metadata": {},
   "source": [
    "Tuning the probability threshold is the most common way to adjust the trade-off between precision and recall, especially in imbalanced datasets like churn prediction.we want to have a high recall which ensures we catch all churners but we will have to pay the price of false alarms which is a trade off we are willing to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf912a40-3c05-4389-8481-2ebd644a584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_proba > t).astype(int)\n",
    "    print(f\"Threshold: {t:.2f} | Precision: {precision_score(y_test, y_pred_t):.2f} | Recall: {recall_score(y_test, y_pred_t):.2f} | F1: {f1_score(y_test, y_pred_t):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2dc431-a222-4e08-a18b-bad9e87a0fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = 0.35  # example from tuning\n",
    "y_pred_final = (y_proba > best_threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d77afc5-7073-4f8b-bf46-2ff3a3210e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_final))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391123ee-ffb2-4315-9b1a-75cdf113fef4",
   "metadata": {},
   "source": [
    "Let's compare the models ROC-AUC and Recall across the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eff187-eb8e-4047-b358-7c00116ccf5b",
   "metadata": {},
   "source": [
    "Our model correctly identifies 84.8% of the actual churners.A high recall will ensure we catch more churners and this important in churn prediction because missing churners (false negatives) can lead to lost revenue.ROC-AUC measures the model’s ability to discriminate between churners and non-churners across all possible thresholds. Our score of 0.917 indicates that if you randomly pick a churner and a non-churner, the model assigns a higher probability of churn to the churner 91.7% of the time.\n",
    "We can conclude that This model is very good at separating churners from non-churners compared to our baseline model and other models too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51bf5b-c441-4c7a-aa01-88fced1b7f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337ceb6-4633-4a78-95d6-f7ba19721e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "feat_importance = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Importance\": best_rf.feature_importances_  # from your fitted GridSearchCV best model\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "print(feat_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878e4e1-3f7b-4ab3-9f26-edc52283b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"viridis\", len(feat_importance))\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feat_importance[\"Feature\"], feat_importance[\"Importance\"], color=palette)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.savefig(\"feature_importance.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb4b75-58ef-4268-bccb-986ee945d49b",
   "metadata": {},
   "source": [
    "## HOW FEATURES INFLUENCING CHURN\n",
    "This are the features that increase rate of churning in the model we seek to deploy:\n",
    "\n",
    "**total charge** Customers with higher total charges are more likely to churn.High spending may indicate dissatisfaction with value or plan costs.\n",
    "\n",
    "**customer service calls** Frequent calls to customer service strongly predict churn. Likely reflects unresolved issues or poor service experience.\n",
    "\n",
    "**total_minute** Customers with higher total minutes usage may be at risk; possibly they are testing services or comparing alternatives.\n",
    "\n",
    "**international plan** Having an international plan increases churn risk. Possibly due to cost or underuse of the plan.\n",
    "\n",
    "**area code** Area code does not influence churn prediction.\n",
    "\n",
    "**state** Geographic location contributes very little."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55216887-e74a-4982-a5c4-b73b3a9e2f70",
   "metadata": {},
   "source": [
    "# 7.CONCLUSION\n",
    "1. New Jersey (NJ) and California (CA) are the states with the highest churning rate at 26.5% churn rate.\n",
    "2. Hawaii (HI) and Alaska (AK) have the most loyal customers with a low rate of 5.7% and 5.8%  churn rate respectively.\n",
    "3. The best performing model has a recall of 84.5% and an ROC-AUC score of 91.7% .\n",
    "4. High charges is the highest feature influencing churn at 0.305\n",
    "5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77596bd-2fdf-4852-b0c9-985910ff8c9a",
   "metadata": {},
   "source": [
    "# 8. RECCOMENDATIONS\n",
    "1. Stakeholders should focus on states with highest churn NJ(New jersey) and CA(Carlifonia) with special offers, loyalty programs, or improved customer service inorder to retain the customers.\n",
    "2. High charges, frequent customer service calls and international plans are the key factors driving churn. By targeting retention efforts to these customers, the company can maximize revenue retention.\n",
    "3. Stakeholders should de-priotize  Voicemail usage, account length, state, and area code because they have minimal predictive value when working on interventions.\n",
    "4. Stakeholders should investigate complaints or service issues in high-churn states to reduce dissatisfaction and also pay attention to areas that are reported by customer complaints to increase customer satisfaction .\n",
    "5. Stakeholders should relocate resources to higher-risk states such as New Jersey, California ,Texas among others since churn is low in those states and replicate strategies used in those states to improve others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eafec26-f1df-432d-a9bd-c2a74c709210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
